{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š Notebook 12: RAGAS Metrics Deep Dive\n",
    "\n",
    "**Understanding How RAG Evaluation Metrics Work Internally**\n",
    "\n",
    "**LangChain 1.0.5+ | RAGAS 0.3.9+ | Mixed Level Class**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand the internal calculation process** for each of the 6 core RAGAS metrics\n",
    "2. **See intermediate outputs** like extracted claims, generated questions, and identified entities\n",
    "3. **Learn to interpret scores with confidence** using threshold guidelines\n",
    "4. **Debug evaluation issues** by understanding what each metric actually measures\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š What Makes This Notebook Different?\n",
    "\n",
    "While Notebook 10 covers **how to use** RAGAS metrics, this notebook goes deeper into **how they work internally**:\n",
    "\n",
    "| Notebook 10 | This Notebook (12) |\n",
    "|-------------|--------------------|\n",
    "| Run evaluation and get scores | See how scores are calculated step-by-step |\n",
    "| Use metrics as black boxes | Understand intermediate outputs |\n",
    "| Focus on results | Focus on the calculation process |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¢ The 6 Metrics We'll Explore\n",
    "\n",
    "| # | Metric | Evaluates | Key Question |\n",
    "|---|--------|-----------|-------------|\n",
    "| 1 | **Faithfulness** | Generator | Is the answer grounded in context? |\n",
    "| 2 | **Answer Relevancy** | Generator | Does the answer address the question? |\n",
    "| 3 | **Context Precision** | Retriever | Are relevant chunks ranked at the top? |\n",
    "| 4 | **Context Recall** | Retriever | Did we retrieve all necessary information? |\n",
    "| 5 | **Context Entity Recall** | Retriever | Did we retrieve all important entities? |\n",
    "| 6 | **Noise Sensitivity** | System | Does noise cause wrong answers? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”° Section 1: Setup & Environment\n",
    "\n",
    "Let's set up our environment with all required imports and configure our LLM/Embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OPENAI_API_KEY found\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"âœ… OPENAI_API_KEY found\")\n",
    "else:\n",
    "    print(\"âŒ OPENAI_API_KEY not found - please set it in your .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Core Imports\n",
    "\n",
    "# Standard library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "# LangChain components\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# RAGAS components\n",
    "from ragas import SingleTurnSample, EvaluationDataset, evaluate\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ResponseRelevancy,\n",
    "    LLMContextPrecisionWithReference,\n",
    "    LLMContextRecall,\n",
    "    ContextEntityRecall,\n",
    "    NoiseSensitivity\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "print(\"âœ… All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM initialized: gpt-4o-mini\n",
      "âœ… Embeddings initialized: text-embedding-3-small\n",
      "âœ… RAGAS wrappers ready\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM and Embeddings\n",
    "\n",
    "# Initialize base models  (OPENAI)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Initialize base models (GEMINI)\n",
    "#llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "#embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "# Wrap for RAGAS compatibility\n",
    "ragas_llm = LangchainLLMWrapper(llm)\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "print(\"âœ… LLM initialized: gpt-4o-mini\")\n",
    "print(\"âœ… Embeddings initialized: text-embedding-3-small\")\n",
    "print(\"âœ… RAGAS wrappers ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Async helper ready\n"
     ]
    }
   ],
   "source": [
    "# Helper function for running async code in Jupyter\n",
    "\n",
    "def run_async(coro):\n",
    "    \"\"\"Helper to run async code in Jupyter notebooks\"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            # We're in Jupyter with an existing loop\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            return loop.run_until_complete(coro)\n",
    "        else:\n",
    "            return asyncio.run(coro)\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "\n",
    "print(\"âœ… Async helper ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ”° Section 2: Faithfulness Deep Dive\n",
    "\n",
    "## What Faithfulness Measures\n",
    "\n",
    "**Faithfulness** checks if the generated answer *sticks to the facts* from the retrieved context. It detects **hallucinations** - when the LLM makes things up that aren't in the source material.\n",
    "\n",
    "### ğŸ“– Analogy\n",
    "\n",
    "> Imagine you're a journalist writing a news story. Faithfulness checks whether everything you wrote can be traced back to your interview notes. If you add details that weren't in your notes, that's a problem!\n",
    "\n",
    "### ğŸ”§ How It Works (3 Steps)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Step 1:        â”‚    â”‚  Step 2:        â”‚    â”‚  Step 3:        â”‚\n",
    "â”‚  Extract Claims â”‚ -> â”‚  Verify Each    â”‚ -> â”‚  Calculate      â”‚\n",
    "â”‚  from Response  â”‚    â”‚  Against Contextâ”‚    â”‚  Score          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ“ Formula\n",
    "\n",
    "$$\\text{Faithfulness} = \\frac{\\text{Number of claims supported by context}}{\\text{Total number of claims}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Step 1: Manual Claim Extraction\n",
    "\n",
    "Let's first see how RAGAS extracts claims from a response. We'll mimic this process manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Response to evaluate:\n",
      "   'The first Super Bowl was held on January 15, 1967 in Los Angeles. It was a sunny day with clear skies.'\n",
      "\n",
      "ğŸ“š Retrieved context:\n",
      "   'The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles, California.'\n"
     ]
    }
   ],
   "source": [
    "# Define our test case\n",
    "\n",
    "# The response we want to evaluate\n",
    "test_response = \"The first Super Bowl was held on January 15, 1967 in Los Angeles. It was a sunny day with clear skies.\"\n",
    "\n",
    "# The context that was retrieved (source of truth)\n",
    "test_context = [\n",
    "    \"The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles, California.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Response to evaluate:\")\n",
    "print(f\"   '{test_response}'\")\n",
    "print(\"\\nğŸ“š Retrieved context:\")\n",
    "print(f\"   '{test_context[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” STEP 1: Extracted Claims from Response\n",
      "==================================================\n",
      "1. The first Super Bowl was held on January 15, 1967.\n",
      "2. The first Super Bowl was held in Los Angeles.\n",
      "3. It was a sunny day on January 15, 1967.\n",
      "4. There were clear skies on January 15, 1967.\n"
     ]
    }
   ],
   "source": [
    "# Manual claim extraction using LLM (mimicking RAGAS)\n",
    "\n",
    "claim_extraction_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the following response, extract ALL factual claims as a numbered list.\n",
    "Each claim should be a single, verifiable statement.\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Extract each factual claim:\n",
    "\"\"\")\n",
    "\n",
    "claim_chain = claim_extraction_prompt | llm | StrOutputParser()\n",
    "\n",
    "extracted_claims_raw = claim_chain.invoke({\"response\": test_response})\n",
    "\n",
    "print(\"ğŸ” STEP 1: Extracted Claims from Response\")\n",
    "print(\"=\" * 50)\n",
    "print(extracted_claims_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Claims to verify:\n",
      "   1. The first Super Bowl was held on January 15, 1967\n",
      "   2. The first Super Bowl was held in Los Angeles\n",
      "   3. It was a sunny day\n",
      "   4. There were clear skies\n"
     ]
    }
   ],
   "source": [
    "# Parse the claims into a list for verification\n",
    "\n",
    "# For our analysis, let's define the claims explicitly\n",
    "claims = [\n",
    "    \"The first Super Bowl was held on January 15, 1967\",\n",
    "    \"The first Super Bowl was held in Los Angeles\",\n",
    "    \"It was a sunny day\",\n",
    "    \"There were clear skies\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“‹ Claims to verify:\")\n",
    "for i, claim in enumerate(claims, 1):\n",
    "    print(f\"   {i}. {claim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Step 2: Claim Verification\n",
    "\n",
    "Now we verify each claim against the retrieved context. This is where hallucinations are detected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” STEP 2: Verifying Each Claim Against Context\n",
      "============================================================\n",
      "\n",
      "âœ… Claim: 'The first Super Bowl was held on January 15, 1967'\n",
      "   Result: SUPPORTED\n",
      "\n",
      "Explanation: The context states that the First AFL-NFL World Championship Game was played...\n",
      "\n",
      "âœ… Claim: 'The first Super Bowl was held in Los Angeles'\n",
      "   Result: SUPPORTED\n",
      "\n",
      "Explanation: The context states that the First AFL-NFL World Championship Game, which is ...\n",
      "\n",
      "âŒ Claim: 'It was a sunny day'\n",
      "   Result: NOT SUPPORTED\n",
      "\n",
      "The context provides information about the date and location of the First AFL-NFL Wor...\n",
      "\n",
      "âŒ Claim: 'There were clear skies'\n",
      "   Result: NOT SUPPORTED\n",
      "\n",
      "The context provides information about the date and location of the First AFL-NFL Wor...\n"
     ]
    }
   ],
   "source": [
    "# Manual claim verification (mimicking RAGAS)\n",
    "\n",
    "verification_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the following context and claim, determine if the claim is SUPPORTED by the context.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Claim: {claim}\n",
    "\n",
    "Answer with:\n",
    "- \"SUPPORTED\" if the claim can be verified from the context\n",
    "- \"NOT SUPPORTED\" if the claim cannot be verified or contradicts the context\n",
    "\n",
    "Also provide a brief explanation.\n",
    "\n",
    "Verdict:\n",
    "\"\"\")\n",
    "\n",
    "verify_chain = verification_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"ğŸ” STEP 2: Verifying Each Claim Against Context\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "verification_results = []\n",
    "for claim in claims:\n",
    "    result = verify_chain.invoke({\n",
    "        \"context\": test_context[0],\n",
    "        \"claim\": claim\n",
    "    })\n",
    "    is_supported = \"SUPPORTED\" in result.upper() and \"NOT SUPPORTED\" not in result.upper()\n",
    "    verification_results.append({\n",
    "        \"claim\": claim,\n",
    "        \"supported\": is_supported,\n",
    "        \"explanation\": result\n",
    "    })\n",
    "    status = \"âœ…\" if is_supported else \"âŒ\"\n",
    "    print(f\"\\n{status} Claim: '{claim}'\")\n",
    "    print(f\"   Result: {result[:100]}...\" if len(result) > 100 else f\"   Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Claim Verification Summary\n",
      "================================================================================\n",
      "                                            Claim Supported?                          Reason\n",
      "The first Super Bowl was held on January 15, 1967      âœ… Yes                Found in context\n",
      "     The first Super Bowl was held in Los Angeles      âœ… Yes                Found in context\n",
      "                               It was a sunny day       âŒ No HALLUCINATION - Not in context!\n",
      "                           There were clear skies       âŒ No HALLUCINATION - Not in context!\n"
     ]
    }
   ],
   "source": [
    "# Display verification results as a table\n",
    "\n",
    "print(\"\\nğŸ“Š Claim Verification Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_verification = pd.DataFrame([\n",
    "    {\n",
    "        \"Claim\": r[\"claim\"],\n",
    "        \"Supported?\": \"âœ… Yes\" if r[\"supported\"] else \"âŒ No\",\n",
    "        \"Reason\": \"Found in context\" if r[\"supported\"] else \"HALLUCINATION - Not in context!\"\n",
    "    }\n",
    "    for r in verification_results\n",
    "])\n",
    "\n",
    "print(df_verification.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Step 3: Calculate Faithfulness Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ STEP 3: Calculate Faithfulness Score\n",
      "==================================================\n",
      "\n",
      "   Supported claims: 2\n",
      "   Total claims: 4\n",
      "\n",
      "   Formula: Faithfulness = 2 / 4\n",
      "\n",
      "   ğŸ“Š Manual Faithfulness Score: 0.50\n"
     ]
    }
   ],
   "source": [
    "# Manual faithfulness calculation\n",
    "\n",
    "supported_count = sum(1 for r in verification_results if r[\"supported\"])\n",
    "total_claims = len(verification_results)\n",
    "\n",
    "manual_faithfulness = supported_count / total_claims\n",
    "\n",
    "print(\"ğŸ”¢ STEP 3: Calculate Faithfulness Score\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Supported claims: {supported_count}\")\n",
    "print(f\"   Total claims: {total_claims}\")\n",
    "print(f\"\\n   Formula: Faithfulness = {supported_count} / {total_claims}\")\n",
    "print(f\"\\n   ğŸ“Š Manual Faithfulness Score: {manual_faithfulness:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Verify with Actual RAGAS Metric\n",
    "\n",
    "Now let's compare our manual calculation with the actual RAGAS Faithfulness metric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ RAGAS Faithfulness Result\n",
      "==================================================\n",
      "\n",
      "   Manual calculation:  0.50\n",
      "   RAGAS metric score:  0.50\n",
      "\n",
      "   Difference: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Run actual RAGAS Faithfulness metric\n",
    "\n",
    "# Create sample in RAGAS format\n",
    "faithfulness_sample = SingleTurnSample(\n",
    "    user_input=\"When was the first Super Bowl?\",\n",
    "    response=test_response,\n",
    "    retrieved_contexts=test_context\n",
    ")\n",
    "\n",
    "# Initialize and run the metric\n",
    "faithfulness_metric = Faithfulness(llm=ragas_llm)\n",
    "\n",
    "ragas_faithfulness = run_async(faithfulness_metric.single_turn_ascore(faithfulness_sample))\n",
    "\n",
    "print(\"ğŸ”¬ RAGAS Faithfulness Result\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Manual calculation:  {manual_faithfulness:.2f}\")\n",
    "print(f\"   RAGAS metric score:  {ragas_faithfulness:.2f}\")\n",
    "print(f\"\\n   Difference: {abs(manual_faithfulness - ragas_faithfulness):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Faithfulness Examples: Good vs Bad\n",
    "\n",
    "Let's see how different types of responses score on Faithfulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Faithfulness Comparison: Different Scenarios\n",
      "======================================================================\n",
      "\n",
      "ğŸ·ï¸  Perfect Faithfulness (No hallucinations)\n",
      "   Response: 'The first Super Bowl was played on January 15, 1967 at the Los Angeles Memorial ...'\n",
      "   Score: 1.00\n",
      "\n",
      "ğŸ·ï¸  Partial Faithfulness (Some hallucinations)\n",
      "   Response: 'The first Super Bowl was on January 15, 1967. The Green Bay Packers won 35-10 wi...'\n",
      "   Score: 0.33\n",
      "\n",
      "ğŸ·ï¸  Zero Faithfulness (Complete hallucination)\n",
      "   Response: 'The first Super Bowl was held in Miami in 1970 and attracted over 100,000 specta...'\n",
      "   Score: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Compare different faithfulness scenarios\n",
    "\n",
    "faithfulness_examples = [\n",
    "    {\n",
    "        \"name\": \"Perfect Faithfulness (No hallucinations)\",\n",
    "        \"response\": \"The first Super Bowl was played on January 15, 1967 at the Los Angeles Memorial Coliseum.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum.\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Partial Faithfulness (Some hallucinations)\",\n",
    "        \"response\": \"The first Super Bowl was on January 15, 1967. The Green Bay Packers won 35-10 with Bart Starr as MVP.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967.\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Zero Faithfulness (Complete hallucination)\",\n",
    "        \"response\": \"The first Super Bowl was held in Miami in 1970 and attracted over 100,000 spectators.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum.\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ“Š Faithfulness Comparison: Different Scenarios\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for example in faithfulness_examples:\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=\"Tell me about the first Super Bowl\",\n",
    "        response=example[\"response\"],\n",
    "        retrieved_contexts=example[\"context\"]\n",
    "    )\n",
    "    score = run_async(faithfulness_metric.single_turn_ascore(sample))\n",
    "    \n",
    "    print(f\"\\nğŸ·ï¸  {example['name']}\")\n",
    "    print(f\"   Response: '{example['response'][:80]}...'\" if len(example['response']) > 80 else f\"   Response: '{example['response']}'\")\n",
    "    print(f\"   Score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Score Interpretation Guide\n",
    "\n",
    "| Score Range | Interpretation | Action |\n",
    "|-------------|----------------|--------|\n",
    "| **0.9 - 1.0** | Excellent - No or minimal hallucinations | âœ… Good to go |\n",
    "| **0.7 - 0.9** | Good - Minor unsupported claims | âš ï¸ Review edge cases |\n",
    "| **0.5 - 0.7** | Concerning - Significant hallucinations | ğŸ”§ Improve prompt/temperature |\n",
    "| **< 0.5** | Poor - Most claims are hallucinated | ğŸš¨ Major fixes needed |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ”° Section 3: Answer Relevancy Deep Dive\n",
    "\n",
    "## What Answer Relevancy Measures\n",
    "\n",
    "**Answer Relevancy** checks if the answer *actually answers* the question asked. It doesn't care if the answer is factually correct - just whether it's relevant to the question.\n",
    "\n",
    "### ğŸ“– Analogy\n",
    "\n",
    "> If someone asks \"What's the capital of France?\" and you answer \"The Eiffel Tower is beautiful,\" your answer might be factually true but completely irrelevant to the question!\n",
    "\n",
    "### ğŸ”§ The \"Reverse Engineering\" Approach\n",
    "\n",
    "RAGAS uses a clever technique: instead of directly comparing the answer to the question, it:\n",
    "\n",
    "1. **Generates hypothetical questions** from the answer (\"What questions would this be a good answer to?\")\n",
    "2. **Compares embeddings** of generated questions with the original question\n",
    "3. **Calculates similarity** - if the generated questions are similar to the original, the answer is relevant!\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Step 1:        â”‚    â”‚  Step 2:        â”‚    â”‚  Step 3:        â”‚\n",
    "â”‚  Generate       â”‚ -> â”‚  Embed All      â”‚ -> â”‚  Calculate      â”‚\n",
    "â”‚  Questions      â”‚    â”‚  Questions      â”‚    â”‚  Similarity     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ“ Formula\n",
    "\n",
    "$$\\text{Answer Relevancy} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{cosine\\_similarity}(E_{generated_i}, E_{original})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Step 1: Hypothetical Question Generation\n",
    "\n",
    "Let's see how RAGAS generates questions from an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Original Question:\n",
      "   'When was the first Super Bowl?'\n",
      "\n",
      "ğŸ“ Answer to Evaluate:\n",
      "   'The first Super Bowl was held on January 15, 1967'\n"
     ]
    }
   ],
   "source": [
    "# Define our test case for relevancy\n",
    "\n",
    "original_question = \"When was the first Super Bowl?\"\n",
    "test_answer = \"The first Super Bowl was held on January 15, 1967\"\n",
    "\n",
    "print(\"ğŸ“ Original Question:\")\n",
    "print(f\"   '{original_question}'\")\n",
    "print(\"\\nğŸ“ Answer to Evaluate:\")\n",
    "print(f\"   '{test_answer}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” STEP 1: Generated Hypothetical Questions\n",
      "==================================================\n",
      "1. When was the inaugural Super Bowl played?  \n",
      "2. What date marks the beginning of the Super Bowl history?  \n",
      "3. Can you tell me when the first Super Bowl took place?  \n"
     ]
    }
   ],
   "source": [
    "# Manual hypothetical question generation (mimicking RAGAS)\n",
    "\n",
    "question_gen_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the following answer, generate exactly 3 different questions that this answer would be a good response to.\n",
    "The questions should be varied but all answerable by this response.\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Generate 3 questions (one per line):\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\"\"\")\n",
    "\n",
    "question_gen_chain = question_gen_prompt | llm | StrOutputParser()\n",
    "\n",
    "generated_questions_raw = question_gen_chain.invoke({\"answer\": test_answer})\n",
    "\n",
    "print(\"ğŸ” STEP 1: Generated Hypothetical Questions\")\n",
    "print(\"=\" * 50)\n",
    "print(generated_questions_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Questions for embedding comparison:\n",
      "   Original: 'When was the first Super Bowl?'\n",
      "   Generated:\n",
      "      1. 'When was the first Super Bowl held?'\n",
      "      2. 'What date was the inaugural Super Bowl?'\n",
      "      3. 'On what day did the first Super Bowl take place?'\n"
     ]
    }
   ],
   "source": [
    "# Parse generated questions (for our calculation)\n",
    "\n",
    "# Manually define likely generated questions\n",
    "generated_questions = [\n",
    "    \"When was the first Super Bowl held?\",\n",
    "    \"What date was the inaugural Super Bowl?\",\n",
    "    \"On what day did the first Super Bowl take place?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“‹ Questions for embedding comparison:\")\n",
    "print(f\"   Original: '{original_question}'\")\n",
    "print(\"   Generated:\")\n",
    "for i, q in enumerate(generated_questions, 1):\n",
    "    print(f\"      {i}. '{q}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Step 2: Embedding and Similarity Calculation\n",
    "\n",
    "Now we compute embeddings and calculate cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cosine similarity function ready\n",
      "\n",
      "ğŸ“ Formula: cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)\n"
     ]
    }
   ],
   "source": [
    "# Define cosine similarity function\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "print(\"âœ… Cosine similarity function ready\")\n",
    "print(\"\\nğŸ“ Formula: cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” STEP 2: Computing Embeddings and Similarities\n",
      "============================================================\n",
      "\n",
      "âœ… Original question embedded (dim=1536)\n",
      "\n",
      "   Question 1: 'When was the first Super Bowl held?'\n",
      "   Similarity to original: 0.9353\n",
      "\n",
      "   Question 2: 'What date was the inaugural Super Bowl?'\n",
      "   Similarity to original: 0.8008\n",
      "\n",
      "   Question 3: 'On what day did the first Super Bowl take place?'\n",
      "   Similarity to original: 0.9063\n"
     ]
    }
   ],
   "source": [
    "# Calculate embeddings and similarities\n",
    "\n",
    "print(\"ğŸ” STEP 2: Computing Embeddings and Similarities\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get embedding for original question\n",
    "original_embedding = embeddings.embed_query(original_question)\n",
    "print(f\"\\nâœ… Original question embedded (dim={len(original_embedding)})\")\n",
    "\n",
    "# Get embeddings for generated questions and calculate similarities\n",
    "similarities = []\n",
    "for i, gen_q in enumerate(generated_questions, 1):\n",
    "    gen_embedding = embeddings.embed_query(gen_q)\n",
    "    sim = cosine_similarity(original_embedding, gen_embedding)\n",
    "    similarities.append(sim)\n",
    "    print(f\"\\n   Question {i}: '{gen_q}'\")\n",
    "    print(f\"   Similarity to original: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Step 3: Calculate Final Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ STEP 3: Calculate Answer Relevancy Score\n",
      "==================================================\n",
      "\n",
      "   Similarities: ['0.9353', '0.8008', '0.9063']\n",
      "   Formula: Average of similarities\n",
      "\n",
      "   (0.9353 + 0.8008 + 0.9063) / 3\n",
      "\n",
      "   ğŸ“Š Manual Answer Relevancy: 0.8808\n"
     ]
    }
   ],
   "source": [
    "# Calculate answer relevancy score\n",
    "\n",
    "manual_relevancy = np.mean(similarities)\n",
    "\n",
    "print(\"ğŸ”¢ STEP 3: Calculate Answer Relevancy Score\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Similarities: {[f'{s:.4f}' for s in similarities]}\")\n",
    "print(f\"   Formula: Average of similarities\")\n",
    "print(f\"\\n   ({' + '.join([f'{s:.4f}' for s in similarities])}) / {len(similarities)}\")\n",
    "print(f\"\\n   ğŸ“Š Manual Answer Relevancy: {manual_relevancy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Verify with Actual RAGAS Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ RAGAS Answer Relevancy Result\n",
      "==================================================\n",
      "\n",
      "   Manual calculation:  0.8808\n",
      "   RAGAS metric score:  0.9353\n"
     ]
    }
   ],
   "source": [
    "# Run actual RAGAS Answer Relevancy metric\n",
    "\n",
    "relevancy_sample = SingleTurnSample(\n",
    "    user_input=original_question,\n",
    "    response=test_answer,\n",
    "    retrieved_contexts=[\"The First AFL-NFL World Championship Game was played on January 15, 1967.\"]\n",
    ")\n",
    "\n",
    "relevancy_metric = ResponseRelevancy(llm=ragas_llm, embeddings=ragas_embeddings)\n",
    "\n",
    "ragas_relevancy = run_async(relevancy_metric.single_turn_ascore(relevancy_sample))\n",
    "\n",
    "print(\"ğŸ”¬ RAGAS Answer Relevancy Result\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Manual calculation:  {manual_relevancy:.4f}\")\n",
    "print(f\"   RAGAS metric score:  {ragas_relevancy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Relevancy Contrast: Good vs Bad Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Answer Relevancy Comparison\n",
      "======================================================================\n",
      "\n",
      "ğŸ·ï¸  Highly Relevant (Directly answers WHEN)\n",
      "   Q: 'When was the first Super Bowl?'\n",
      "   A: 'The first Super Bowl was held on January 15, 1967.'\n",
      "   Score: 0.9353\n",
      "\n",
      "ğŸ·ï¸  Partially Relevant (Answers but adds extra info)\n",
      "   Q: 'When was the first Super Bowl?'\n",
      "   A: 'The Super Bowl is the annual championship game of the NFL, f...'\n",
      "   Score: 0.7254\n",
      "\n",
      "ğŸ·ï¸  Low Relevancy (Doesn't answer WHEN)\n",
      "   Q: 'When was the first Super Bowl?'\n",
      "   A: 'The Super Bowl is the annual championship game of the Nation...'\n",
      "   Score: 0.6773\n",
      "\n",
      "ğŸ·ï¸  Off-topic (Completely irrelevant)\n",
      "   Q: 'When was the first Super Bowl?'\n",
      "   A: 'Pizza is a popular Italian dish that spread worldwide in the...'\n",
      "   Score: 0.1667\n"
     ]
    }
   ],
   "source": [
    "# Compare different relevancy scenarios\n",
    "\n",
    "relevancy_examples = [\n",
    "    {\n",
    "        \"name\": \"Highly Relevant (Directly answers WHEN)\",\n",
    "        \"question\": \"When was the first Super Bowl?\",\n",
    "        \"answer\": \"The first Super Bowl was held on January 15, 1967.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Partially Relevant (Answers but adds extra info)\",\n",
    "        \"question\": \"When was the first Super Bowl?\",\n",
    "        \"answer\": \"The Super Bowl is the annual championship game of the NFL, first held on January 15, 1967.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Low Relevancy (Doesn't answer WHEN)\",\n",
    "        \"question\": \"When was the first Super Bowl?\",\n",
    "        \"answer\": \"The Super Bowl is the annual championship game of the National Football League.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Off-topic (Completely irrelevant)\",\n",
    "        \"question\": \"When was the first Super Bowl?\",\n",
    "        \"answer\": \"Pizza is a popular Italian dish that spread worldwide in the 20th century.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ“Š Answer Relevancy Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for example in relevancy_examples:\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=example[\"question\"],\n",
    "        response=example[\"answer\"],\n",
    "        retrieved_contexts=[\"Context not relevant for this metric.\"]\n",
    "    )\n",
    "    score = run_async(relevancy_metric.single_turn_ascore(sample))\n",
    "    \n",
    "    print(f\"\\nğŸ·ï¸  {example['name']}\")\n",
    "    print(f\"   Q: '{example['question']}'\")\n",
    "    print(f\"   A: '{example['answer'][:60]}...'\" if len(example['answer']) > 60 else f\"   A: '{example['answer']}'\")\n",
    "    print(f\"   Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Score Interpretation Guide\n",
    "\n",
    "| Score Range | Interpretation | Example |\n",
    "|-------------|----------------|--------|\n",
    "| **0.9 - 1.0** | Directly addresses the question | \"When?\" â†’ \"January 15, 1967\" |\n",
    "| **0.7 - 0.9** | Mostly relevant with some tangents | \"When?\" â†’ \"It was 1967, a historic game\" |\n",
    "| **0.4 - 0.7** | Partially relevant, missing key aspects | \"When?\" â†’ \"It's an NFL championship\" |\n",
    "| **< 0.4** | Off-topic or doesn't answer the question | \"When?\" â†’ \"Pizza is delicious\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ”° Section 4: Context Precision Deep Dive\n",
    "\n",
    "## What Context Precision Measures\n",
    "\n",
    "**Context Precision** evaluates whether the *most relevant chunks are ranked at the top* of your retrieval results. It's about **ranking quality**, not just whether you retrieved relevant information.\n",
    "\n",
    "### ğŸ“– Analogy\n",
    "\n",
    "> Imagine you're a librarian handing someone 5 books to answer their question. Context Precision asks: \"Did you put the most useful book on top of the pile?\"\n",
    "\n",
    "### ğŸ”§ How It Works\n",
    "\n",
    "1. For each retrieved chunk, determine if it's **relevant** to the question/reference\n",
    "2. Calculate **Precision@K** at each position (weighted by position)\n",
    "3. Relevant chunks at the **top** = higher score\n",
    "\n",
    "### ğŸ“ Formula\n",
    "\n",
    "$$\\text{Context Precision@K} = \\frac{\\sum_{k=1}^{K} (\\text{Precision@k} \\times \\text{relevance}_k)}{\\text{Total relevant items in top K}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Understanding Ranking Impact\n",
    "\n",
    "Let's see how the **same chunks in different orders** produce different scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Question: 'Where is the Eiffel Tower located?'\n",
      "\n",
      "ğŸ“š Retrieved Chunks (with relevance):\n",
      "   1. âœ… Relevant: 'The Eiffel Tower is located in Paris, France.'\n",
      "   2. âœ… Relevant: 'Paris is the capital of France.'\n",
      "   3. âŒ Not relevant: 'The tower was built in 1889.'\n",
      "   4. âŒ Not relevant: 'Pizza originated in Italy.'\n"
     ]
    }
   ],
   "source": [
    "# Define our test case for context precision\n",
    "\n",
    "question = \"Where is the Eiffel Tower located?\"\n",
    "reference = \"The Eiffel Tower is located in Paris, France.\"\n",
    "\n",
    "# Chunks with known relevance\n",
    "chunks_with_relevance = [\n",
    "    (\"The Eiffel Tower is located in Paris, France.\", True),      # Directly relevant\n",
    "    (\"Paris is the capital of France.\", True),                     # Somewhat relevant\n",
    "    (\"The tower was built in 1889.\", False),                       # Not relevant to WHERE\n",
    "    (\"Pizza originated in Italy.\", False),                         # Completely irrelevant\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Question: '{}'\\n\".format(question))\n",
    "print(\"ğŸ“š Retrieved Chunks (with relevance):\")\n",
    "for i, (chunk, relevant) in enumerate(chunks_with_relevance, 1):\n",
    "    status = \"âœ… Relevant\" if relevant else \"âŒ Not relevant\"\n",
    "    print(f\"   {i}. {status}: '{chunk}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Manual Relevance Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Manual Relevance Classification\n",
      "============================================================\n",
      "âœ… 'The Eiffel Tower is located in Paris, France....' â†’ RELEVANT\n",
      "âœ… 'Paris is the capital of France....' â†’ RELEVANT\n",
      "âŒ 'The tower was built in 1889....' â†’ NOT RELEVANT\n",
      "âŒ 'Pizza originated in Italy....' â†’ NOT RELEVANT\n"
     ]
    }
   ],
   "source": [
    "# Manual relevance classification using LLM\n",
    "\n",
    "relevance_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the question and reference answer, determine if the following context chunk is RELEVANT.\n",
    "\n",
    "Question: {question}\n",
    "Reference Answer: {reference}\n",
    "Context Chunk: {chunk}\n",
    "\n",
    "Is this chunk relevant for answering the question? Answer only \"RELEVANT\" or \"NOT RELEVANT\".\n",
    "\"\"\")\n",
    "\n",
    "relevance_chain = relevance_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"ğŸ” Manual Relevance Classification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "relevance_results = []\n",
    "for chunk, expected in chunks_with_relevance:\n",
    "    result = relevance_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"reference\": reference,\n",
    "        \"chunk\": chunk\n",
    "    })\n",
    "    is_relevant = \"RELEVANT\" in result.upper() and \"NOT RELEVANT\" not in result.upper()\n",
    "    relevance_results.append(is_relevant)\n",
    "    status = \"âœ…\" if is_relevant else \"âŒ\"\n",
    "    print(f\"{status} '{chunk[:50]}...' â†’ {result.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Precision@K Calculation: Good Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š GOOD RANKING: Relevant chunks at TOP\n",
      "============================================================\n",
      "\n",
      "Ranking: [âœ… Relevant, âœ… Relevant, âŒ Not Rel, âŒ Not Rel]\n",
      "\n",
      "Precision@K calculation:\n",
      "   Position 1: Precision@1 = 1/1 = 1.00 â†’ Contributes\n",
      "   Position 2: Precision@2 = 2/2 = 1.00 â†’ Contributes\n",
      "   Position 3: Precision@3 = 2/3 = 0.67 â†’ Does NOT contribute\n",
      "   Position 4: Precision@4 = 2/4 = 0.50 â†’ Does NOT contribute\n",
      "\n",
      "   Sum of contributing precisions: 2.00\n",
      "   Total relevant items: 2\n",
      "\n",
      "   ğŸ“Š Context Precision (Good Ranking): 1.00\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision@K for GOOD ranking (relevant at top)\n",
    "\n",
    "# Good ranking: [Relevant, Relevant, Not Relevant, Not Relevant]\n",
    "good_ranking = [True, True, False, False]\n",
    "\n",
    "print(\"ğŸ“Š GOOD RANKING: Relevant chunks at TOP\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nRanking: [âœ… Relevant, âœ… Relevant, âŒ Not Rel, âŒ Not Rel]\")\n",
    "print(\"\\nPrecision@K calculation:\")\n",
    "\n",
    "precisions_good = []\n",
    "relevant_count = 0\n",
    "for k, is_relevant in enumerate(good_ranking, 1):\n",
    "    if is_relevant:\n",
    "        relevant_count += 1\n",
    "    precision_at_k = relevant_count / k\n",
    "    contributes = \"â†’ Contributes\" if is_relevant else \"â†’ Does NOT contribute\"\n",
    "    print(f\"   Position {k}: Precision@{k} = {relevant_count}/{k} = {precision_at_k:.2f} {contributes}\")\n",
    "    if is_relevant:\n",
    "        precisions_good.append(precision_at_k)\n",
    "\n",
    "total_relevant = sum(good_ranking)\n",
    "context_precision_good = sum(precisions_good) / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "print(f\"\\n   Sum of contributing precisions: {sum(precisions_good):.2f}\")\n",
    "print(f\"   Total relevant items: {total_relevant}\")\n",
    "print(f\"\\n   ğŸ“Š Context Precision (Good Ranking): {context_precision_good:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š BAD RANKING: Relevant chunks at BOTTOM\n",
      "============================================================\n",
      "\n",
      "Ranking: [âŒ Not Rel, âŒ Not Rel, âœ… Relevant, âœ… Relevant]\n",
      "\n",
      "Precision@K calculation:\n",
      "   Position 1: Precision@1 = 0/1 = 0.00 â†’ Does NOT contribute\n",
      "   Position 2: Precision@2 = 0/2 = 0.00 â†’ Does NOT contribute\n",
      "   Position 3: Precision@3 = 1/3 = 0.33 â†’ Contributes\n",
      "   Position 4: Precision@4 = 2/4 = 0.50 â†’ Contributes\n",
      "\n",
      "   Sum of contributing precisions: 0.83\n",
      "   Total relevant items: 2\n",
      "\n",
      "   ğŸ“Š Context Precision (Bad Ranking): 0.42\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision@K for BAD ranking (relevant at bottom)\n",
    "\n",
    "# Bad ranking: [Not Relevant, Not Relevant, Relevant, Relevant]\n",
    "bad_ranking = [False, False, True, True]\n",
    "\n",
    "print(\"ğŸ“Š BAD RANKING: Relevant chunks at BOTTOM\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nRanking: [âŒ Not Rel, âŒ Not Rel, âœ… Relevant, âœ… Relevant]\")\n",
    "print(\"\\nPrecision@K calculation:\")\n",
    "\n",
    "precisions_bad = []\n",
    "relevant_count = 0\n",
    "for k, is_relevant in enumerate(bad_ranking, 1):\n",
    "    if is_relevant:\n",
    "        relevant_count += 1\n",
    "    precision_at_k = relevant_count / k\n",
    "    contributes = \"â†’ Contributes\" if is_relevant else \"â†’ Does NOT contribute\"\n",
    "    print(f\"   Position {k}: Precision@{k} = {relevant_count}/{k} = {precision_at_k:.2f} {contributes}\")\n",
    "    if is_relevant:\n",
    "        precisions_bad.append(precision_at_k)\n",
    "\n",
    "total_relevant = sum(bad_ranking)\n",
    "context_precision_bad = sum(precisions_bad) / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "print(f\"\\n   Sum of contributing precisions: {sum(precisions_bad):.2f}\")\n",
    "print(f\"   Total relevant items: {total_relevant}\")\n",
    "print(f\"\\n   ğŸ“Š Context Precision (Bad Ranking): {context_precision_bad:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š RANKING COMPARISON\n",
      "============================================================\n",
      "\n",
      "GOOD RANKING (Score: 1.00)          BAD RANKING (Score: 0.42)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ 1. âœ… Eiffel Tower Paris â”‚          â”‚ 1. âŒ Pizza Italy        â”‚\n",
      "â”‚ 2. âœ… Paris is capital   â”‚          â”‚ 2. âŒ Built in 1889      â”‚\n",
      "â”‚ 3. âŒ Built in 1889      â”‚          â”‚ 3. âœ… Paris is capital   â”‚\n",
      "â”‚ 4. âŒ Pizza Italy        â”‚          â”‚ 4. âœ… Eiffel Tower Paris â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "     Relevant at TOP! âœ“                  Relevant at BOTTOM! âœ—\n",
      "\n",
      "   Difference: 0.58\n",
      "   Same chunks, different ranking â†’ HUGE difference in score!\n"
     ]
    }
   ],
   "source": [
    "# Visual comparison\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š RANKING COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "GOOD RANKING (Score: {:.2f})          BAD RANKING (Score: {:.2f})\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 1. âœ… Eiffel Tower Paris â”‚          â”‚ 1. âŒ Pizza Italy        â”‚\n",
    "â”‚ 2. âœ… Paris is capital   â”‚          â”‚ 2. âŒ Built in 1889      â”‚\n",
    "â”‚ 3. âŒ Built in 1889      â”‚          â”‚ 3. âœ… Paris is capital   â”‚\n",
    "â”‚ 4. âŒ Pizza Italy        â”‚          â”‚ 4. âœ… Eiffel Tower Paris â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     Relevant at TOP! âœ“                  Relevant at BOTTOM! âœ—\n",
    "\"\"\".format(context_precision_good, context_precision_bad))\n",
    "\n",
    "print(f\"   Difference: {context_precision_good - context_precision_bad:.2f}\")\n",
    "print(\"   Same chunks, different ranking â†’ HUGE difference in score!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Verify with RAGAS Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ RAGAS Context Precision Results\n",
      "==================================================\n",
      "\n",
      "   Good Ranking (relevant at top): 1.00\n",
      "   Bad Ranking (relevant at bottom): 0.42\n",
      "\n",
      "   Difference: 0.58\n"
     ]
    }
   ],
   "source": [
    "# Run actual RAGAS Context Precision\n",
    "\n",
    "# Good ranking sample\n",
    "good_sample = SingleTurnSample(\n",
    "    user_input=question,\n",
    "    reference=reference,\n",
    "    retrieved_contexts=[\n",
    "        \"The Eiffel Tower is located in Paris, France.\",\n",
    "        \"Paris is the capital of France.\",\n",
    "        \"The tower was built in 1889.\",\n",
    "        \"Pizza originated in Italy.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Bad ranking sample (same chunks, reversed order)\n",
    "bad_sample = SingleTurnSample(\n",
    "    user_input=question,\n",
    "    reference=reference,\n",
    "    retrieved_contexts=[\n",
    "        \"Pizza originated in Italy.\",\n",
    "        \"The tower was built in 1889.\",\n",
    "        \"Paris is the capital of France.\",\n",
    "        \"The Eiffel Tower is located in Paris, France.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "precision_metric = LLMContextPrecisionWithReference(llm=ragas_llm)\n",
    "\n",
    "good_score = run_async(precision_metric.single_turn_ascore(good_sample))\n",
    "bad_score = run_async(precision_metric.single_turn_ascore(bad_sample))\n",
    "\n",
    "print(\"ğŸ”¬ RAGAS Context Precision Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Good Ranking (relevant at top): {good_score:.2f}\")\n",
    "print(f\"   Bad Ranking (relevant at bottom): {bad_score:.2f}\")\n",
    "print(f\"\\n   Difference: {good_score - bad_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ Section 5: Context Recall Deep Dive\n",
    "\n",
    "## What Context Recall Measures\n",
    "\n",
    "**Context Recall** checks if you retrieved *all the necessary information* to answer the question. It measures **retrieval completeness**.\n",
    "\n",
    "### ğŸ“– Analogy\n",
    "\n",
    "> You're studying for an exam using a textbook. Context Recall asks: \"Did you read all the chapters needed to answer every exam question, or did you skip some important ones?\"\n",
    "\n",
    "### ğŸ”§ How It Works\n",
    "\n",
    "1. Break down the **reference answer** into individual claims\n",
    "2. Check if each claim can be **attributed** to the retrieved context\n",
    "3. Calculate: claims found / total claims\n",
    "\n",
    "### ğŸ“ Formula\n",
    "\n",
    "$$\\text{Context Recall} = \\frac{\\text{Reference claims found in context}}{\\text{Total claims in reference}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Reference Answer (Ground Truth):\n",
      "   'The Eiffel Tower is located in Paris. It was built in 1889. It is 330 meters tall.'\n",
      "\n",
      "ğŸ“š Retrieved Context:\n",
      "   1. 'The Eiffel Tower is a landmark located in Paris, France.'\n",
      "   2. 'The tower was completed in 1889 for the World's Fair.'\n"
     ]
    }
   ],
   "source": [
    "# Context Recall example setup\n",
    "\n",
    "recall_question = \"Tell me about the Eiffel Tower.\"\n",
    "recall_reference = \"The Eiffel Tower is located in Paris. It was built in 1889. It is 330 meters tall.\"\n",
    "\n",
    "# Retrieved context (missing the height information)\n",
    "recall_context = [\n",
    "    \"The Eiffel Tower is a landmark located in Paris, France.\",\n",
    "    \"The tower was completed in 1889 for the World's Fair.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Reference Answer (Ground Truth):\")\n",
    "print(f\"   '{recall_reference}'\")\n",
    "print(\"\\nğŸ“š Retrieved Context:\")\n",
    "for i, ctx in enumerate(recall_context, 1):\n",
    "    print(f\"   {i}. '{ctx}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” STEP 1: Reference Claims\n",
      "==================================================\n",
      "   1. The Eiffel Tower is located in Paris\n",
      "   2. It was built in 1889\n",
      "   3. It is 330 meters tall\n"
     ]
    }
   ],
   "source": [
    "# Extract claims from reference\n",
    "\n",
    "reference_claims = [\n",
    "    \"The Eiffel Tower is located in Paris\",\n",
    "    \"It was built in 1889\",\n",
    "    \"It is 330 meters tall\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ” STEP 1: Reference Claims\")\n",
    "print(\"=\" * 50)\n",
    "for i, claim in enumerate(reference_claims, 1):\n",
    "    print(f\"   {i}. {claim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” STEP 2: Claim Attribution Check\n",
      "============================================================\n",
      "   âœ… Found: 'The Eiffel Tower is located in Paris'\n",
      "   âœ… Found: 'It was built in 1889'\n",
      "   âŒ MISSING: 'It is 330 meters tall'\n",
      "      âš ï¸ This information was NOT retrieved!\n"
     ]
    }
   ],
   "source": [
    "# Check attribution of each claim\n",
    "\n",
    "attribution_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Can the following claim be attributed to (found in) the given context?\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Claim: {claim}\n",
    "\n",
    "Answer \"YES\" if the claim is supported by the context, \"NO\" if it cannot be found.\n",
    "\"\"\")\n",
    "\n",
    "attribution_chain = attribution_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"ğŸ” STEP 2: Claim Attribution Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "combined_context = \"\\n\".join(recall_context)\n",
    "attribution_results = []\n",
    "\n",
    "for claim in reference_claims:\n",
    "    result = attribution_chain.invoke({\n",
    "        \"context\": combined_context,\n",
    "        \"claim\": claim\n",
    "    })\n",
    "    found = \"YES\" in result.upper()\n",
    "    attribution_results.append(found)\n",
    "    status = \"âœ… Found\" if found else \"âŒ MISSING\"\n",
    "    print(f\"   {status}: '{claim}'\")\n",
    "    if not found:\n",
    "        print(f\"      âš ï¸ This information was NOT retrieved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ STEP 3: Calculate Context Recall\n",
      "==================================================\n",
      "\n",
      "   Claims found in context: 2\n",
      "   Total claims in reference: 3\n",
      "\n",
      "   Formula: 2 / 3 = 0.67\n",
      "\n",
      "   ğŸ“Š Context Recall: 0.67\n",
      "\n",
      "   âš ï¸ Interpretation: 33% of required info was NOT retrieved!\n"
     ]
    }
   ],
   "source": [
    "# Calculate Context Recall\n",
    "\n",
    "claims_found = sum(attribution_results)\n",
    "total_claims = len(reference_claims)\n",
    "manual_recall = claims_found / total_claims\n",
    "\n",
    "print(\"ğŸ”¢ STEP 3: Calculate Context Recall\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Claims found in context: {claims_found}\")\n",
    "print(f\"   Total claims in reference: {total_claims}\")\n",
    "print(f\"\\n   Formula: {claims_found} / {total_claims} = {manual_recall:.2f}\")\n",
    "print(f\"\\n   ğŸ“Š Context Recall: {manual_recall:.2f}\")\n",
    "print(f\"\\n   âš ï¸ Interpretation: {100 - manual_recall*100:.0f}% of required info was NOT retrieved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ RAGAS Context Recall Result\n",
      "==================================================\n",
      "\n",
      "   Manual calculation: 0.67\n",
      "   RAGAS metric score: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Verify with RAGAS\n",
    "\n",
    "recall_sample = SingleTurnSample(\n",
    "    user_input=recall_question,\n",
    "    response=\"The Eiffel Tower is in Paris and was built in 1889.\",\n",
    "    reference=recall_reference,\n",
    "    retrieved_contexts=recall_context\n",
    ")\n",
    "\n",
    "recall_metric = LLMContextRecall(llm=ragas_llm)\n",
    "ragas_recall = run_async(recall_metric.single_turn_ascore(recall_sample))\n",
    "\n",
    "print(\"ğŸ”¬ RAGAS Context Recall Result\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Manual calculation: {manual_recall:.2f}\")\n",
    "print(f\"   RAGAS metric score: {ragas_recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ Section 6: Context Entity Recall Deep Dive\n",
    "\n",
    "## What Context Entity Recall Measures\n",
    "\n",
    "**Context Entity Recall** checks if you retrieved context containing all the *important entities* (people, places, dates, organizations) mentioned in the reference answer.\n",
    "\n",
    "### ğŸ“– Analogy\n",
    "\n",
    "> If the correct answer mentions \"Einstein,\" \"1905,\" and \"Princeton,\" did your retrieved documents mention these specific entities?\n",
    "\n",
    "### ğŸ“ Formula\n",
    "\n",
    "$$\\text{Entity Recall} = \\frac{\\text{Entities in both reference AND context}}{\\text{Total entities in reference}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Reference Answer:\n",
      "   'Albert Einstein developed the theory of relativity at Princeton University in 1905.'\n",
      "\n",
      "ğŸ“š Retrieved Context:\n",
      "   'Albert Einstein was a famous physicist who worked at Princeton.'\n"
     ]
    }
   ],
   "source": [
    "# Entity Recall example setup\n",
    "\n",
    "entity_reference = \"Albert Einstein developed the theory of relativity at Princeton University in 1905.\"\n",
    "entity_context = [\n",
    "    \"Albert Einstein was a famous physicist who worked at Princeton.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Reference Answer:\")\n",
    "print(f\"   '{entity_reference}'\")\n",
    "print(\"\\nğŸ“š Retrieved Context:\")\n",
    "print(f\"   '{entity_context[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 33: Manual entity extraction\u001b[39;00m\n\u001b[32m      3\u001b[39m entity_extraction_prompt = ChatPromptTemplate.from_template(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mExtract all named entities from the following text. \u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33mInclude: PERSON, ORGANIZATION, LOCATION, DATE, and other proper nouns.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[33mList each entity on a new line with its type:\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33m\"\"\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m entity_chain = entity_extraction_prompt | \u001b[43mllm\u001b[49m | StrOutputParser()\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ” Entity Extraction\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "# Manual entity extraction\n",
    "\n",
    "entity_extraction_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Extract all named entities from the following text. \n",
    "Include: PERSON, ORGANIZATION, LOCATION, DATE, and other proper nouns.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "List each entity on a new line with its type:\n",
    "\"\"\")\n",
    "\n",
    "entity_chain = entity_extraction_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"ğŸ” Entity Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ“‹ Reference Entities:\")\n",
    "ref_entities = entity_chain.invoke({\"text\": entity_reference})\n",
    "print(ref_entities)\n",
    "\n",
    "print(\"\\nğŸ“‹ Context Entities:\")\n",
    "ctx_entities = entity_chain.invoke({\"text\": entity_context[0]})\n",
    "print(ctx_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Entity Comparison\n",
      "============================================================\n",
      "\n",
      "| Entity in Reference | Type | Found in Context? |\n",
      "|--------------------|--------------|------------------|\n",
      "| Albert Einstein    | PERSON       | âœ… Yes            |\n",
      "| Princeton University | ORGANIZATION | âœ… Yes            |\n",
      "| 1905               | DATE         | âŒ MISSING        |\n",
      "\n",
      "ğŸ“Š Entity Recall: 2/3 = 0.67\n",
      "âš ï¸ Missing: '1905' - Critical date not retrieved!\n"
     ]
    }
   ],
   "source": [
    "# Manual entity analysis\n",
    "\n",
    "# Define entities for analysis\n",
    "reference_entities = {\n",
    "    \"Albert Einstein\": \"PERSON\",\n",
    "    \"Princeton University\": \"ORGANIZATION\",\n",
    "    \"1905\": \"DATE\"\n",
    "}\n",
    "\n",
    "context_entities = {\n",
    "    \"Albert Einstein\": \"PERSON\",\n",
    "    \"Princeton\": \"ORGANIZATION\"  # Partial match\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š Entity Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n| Entity in Reference | Type | Found in Context? |\")\n",
    "print(\"|\" + \"-\" * 20 + \"|\" + \"-\" * 14 + \"|\" + \"-\" * 18 + \"|\")\n",
    "\n",
    "found_count = 0\n",
    "for entity, entity_type in reference_entities.items():\n",
    "    # Check if entity (or partial) exists in context\n",
    "    found = any(entity.lower() in ctx.lower() or ctx.lower() in entity.lower() \n",
    "                for ctx in context_entities.keys())\n",
    "    if found:\n",
    "        found_count += 1\n",
    "    status = \"âœ… Yes\" if found else \"âŒ MISSING\"\n",
    "    print(f\"| {entity:18} | {entity_type:12} | {status:16} |\")\n",
    "\n",
    "entity_recall = found_count / len(reference_entities)\n",
    "print(f\"\\nğŸ“Š Entity Recall: {found_count}/{len(reference_entities)} = {entity_recall:.2f}\")\n",
    "print(f\"âš ï¸ Missing: '1905' - Critical date not retrieved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ RAGAS Context Entity Recall Result\n",
      "==================================================\n",
      "\n",
      "   Manual estimate: 0.67\n",
      "   RAGAS metric score: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Verify with RAGAS\n",
    "\n",
    "entity_sample = SingleTurnSample(\n",
    "    reference=entity_reference,\n",
    "    retrieved_contexts=entity_context\n",
    ")\n",
    "\n",
    "entity_metric = ContextEntityRecall(llm=ragas_llm)\n",
    "ragas_entity_recall = run_async(entity_metric.single_turn_ascore(entity_sample))\n",
    "\n",
    "print(\"ğŸ”¬ RAGAS Context Entity Recall Result\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Manual estimate: {entity_recall:.2f}\")\n",
    "print(f\"   RAGAS metric score: {ragas_entity_recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Types Tracked\n",
    "\n",
    "```\n",
    "PERSON:       Albert Einstein, Marie Curie, Elon Musk\n",
    "ORGANIZATION: Princeton University, NASA, Google\n",
    "LOCATION:     Paris, Mount Everest, Pacific Ocean\n",
    "DATE:         1905, January 15, 20th century\n",
    "NUMBER:       330 meters, $1 billion, 99.9%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ Section 7: Noise Sensitivity Deep Dive\n",
    "\n",
    "## What Noise Sensitivity Measures\n",
    "\n",
    "**Noise Sensitivity** tests how much *irrelevant information* in the retrieved context causes errors in the answer. It measures **robustness** to noise.\n",
    "\n",
    "### ğŸ“– Analogy\n",
    "\n",
    "> You're taking an open-book exam, but someone mixed random Wikipedia articles into your notes. Noise Sensitivity measures how often those random articles cause you to write wrong answers.\n",
    "\n",
    "### âš ï¸ Important: Lower is Better!\n",
    "\n",
    "Unlike other metrics where higher is better, for Noise Sensitivity:\n",
    "- **0.0** = Great! Model ignores noise completely\n",
    "- **1.0** = Bad! Model is very easily confused by irrelevant information\n",
    "\n",
    "### ğŸ“ Formula\n",
    "\n",
    "$$\\text{Noise Sensitivity} = \\frac{\\text{Incorrect claims from noisy context}}{\\text{Total claims}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Question: 'What is LIC known for?'\n",
      "\n",
      "ğŸ“ Response to evaluate:\n",
      "   'LIC is the largest insurance company in India, known for its vast portfolio. LIC contributes to financial stability.'\n",
      "\n",
      "ğŸ“ Reference (Ground Truth):\n",
      "   'LIC is the largest insurance company in India, established in 1956. It is known for managing a large portfolio of investments.'\n",
      "\n",
      "ğŸ“š Retrieved Contexts:\n",
      "   1. 'LIC was established in 1956 following nationalization....' âœ…\n",
      "   2. 'LIC is the largest insurance company with huge investments....' âœ…\n",
      "   3. 'LIC manages substantial funds for financial stability....' âœ…\n",
      "   4. 'The Indian economy is one of the fastest-growing economies.....' â† NOISE!\n"
     ]
    }
   ],
   "source": [
    "# Noise Sensitivity example setup\n",
    "\n",
    "noise_question = \"What is LIC known for?\"\n",
    "noise_response = \"LIC is the largest insurance company in India, known for its vast portfolio. LIC contributes to financial stability.\"\n",
    "noise_reference = \"LIC is the largest insurance company in India, established in 1956. It is known for managing a large portfolio of investments.\"\n",
    "\n",
    "noise_contexts = [\n",
    "    \"LIC was established in 1956 following nationalization.\",           # âœ… Relevant\n",
    "    \"LIC is the largest insurance company with huge investments.\",      # âœ… Relevant\n",
    "    \"LIC manages substantial funds for financial stability.\",           # âœ… Relevant\n",
    "    \"The Indian economy is one of the fastest-growing economies...\"     # âŒ NOISE!\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Question: '{}'\\n\".format(noise_question))\n",
    "print(\"ğŸ“ Response to evaluate:\")\n",
    "print(f\"   '{noise_response}'\")\n",
    "print(\"\\nğŸ“ Reference (Ground Truth):\")\n",
    "print(f\"   '{noise_reference}'\")\n",
    "print(\"\\nğŸ“š Retrieved Contexts:\")\n",
    "for i, ctx in enumerate(noise_contexts, 1):\n",
    "    noise_tag = \" â† NOISE!\" if i == 4 else \" âœ…\"\n",
    "    print(f\"   {i}. '{ctx[:60]}...'{noise_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Claim Analysis\n",
      "======================================================================\n",
      "\n",
      "| Claim | Correct? | Reason |\n",
      "|---------------------------------------------|----------|----------------------------------------|\n",
      "| LIC is the largest insurance company in Ind | âœ… Yes    | Matches reference                      |\n",
      "| LIC is known for its vast portfolio         | âœ… Yes    | Matches reference (portfolio)          |\n",
      "| LIC contributes to financial stability      | âŒ No     | NOT in reference - possible hallucinat |\n"
     ]
    }
   ],
   "source": [
    "# Analyze claims in response\n",
    "\n",
    "response_claims = [\n",
    "    (\"LIC is the largest insurance company in India\", True, \"Matches reference\"),\n",
    "    (\"LIC is known for its vast portfolio\", True, \"Matches reference (portfolio)\"),\n",
    "    (\"LIC contributes to financial stability\", False, \"NOT in reference - possible hallucination from noise!\")\n",
    "]\n",
    "\n",
    "print(\"ğŸ” Claim Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n| Claim | Correct? | Reason |\")\n",
    "print(\"|\" + \"-\" * 45 + \"|\" + \"-\" * 10 + \"|\" + \"-\" * 40 + \"|\")\n",
    "\n",
    "incorrect_count = 0\n",
    "for claim, is_correct, reason in response_claims:\n",
    "    status = \"âœ… Yes\" if is_correct else \"âŒ No\"\n",
    "    if not is_correct:\n",
    "        incorrect_count += 1\n",
    "    print(f\"| {claim[:43]:43} | {status:8} | {reason[:38]:38} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Noise Sensitivity Calculation\n",
      "==================================================\n",
      "\n",
      "   Incorrect claims: 1\n",
      "   Total claims: 3\n",
      "\n",
      "   Formula: 1 / 3 = 0.33\n",
      "\n",
      "   ğŸ“Š Noise Sensitivity: 0.33\n",
      "   âš ï¸ Warning! Model is sometimes confused by noise.\n"
     ]
    }
   ],
   "source": [
    "# Calculate Noise Sensitivity\n",
    "\n",
    "total_claims = len(response_claims)\n",
    "noise_sensitivity = incorrect_count / total_claims\n",
    "\n",
    "print(\"ğŸ”¢ Noise Sensitivity Calculation\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Incorrect claims: {incorrect_count}\")\n",
    "print(f\"   Total claims: {total_claims}\")\n",
    "print(f\"\\n   Formula: {incorrect_count} / {total_claims} = {noise_sensitivity:.2f}\")\n",
    "print(f\"\\n   ğŸ“Š Noise Sensitivity: {noise_sensitivity:.2f}\")\n",
    "\n",
    "if noise_sensitivity < 0.3:\n",
    "    print(\"   âœ… Good! Model is mostly resistant to noise.\")\n",
    "elif noise_sensitivity < 0.6:\n",
    "    print(\"   âš ï¸ Warning! Model is sometimes confused by noise.\")\n",
    "else:\n",
    "    print(\"   ğŸš¨ Bad! Model is highly susceptible to noise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ RAGAS Noise Sensitivity Result\n",
      "==================================================\n",
      "\n",
      "   Mode: relevant\n",
      "   Score: 0.33\n",
      "\n",
      "   Remember: Lower is better for this metric!\n"
     ]
    }
   ],
   "source": [
    "# Verify with RAGAS (both modes)\n",
    "\n",
    "noise_sample = SingleTurnSample(\n",
    "    user_input=noise_question,\n",
    "    response=noise_response,\n",
    "    reference=noise_reference,\n",
    "    retrieved_contexts=noise_contexts\n",
    ")\n",
    "\n",
    "# Relevant mode: errors from relevant contexts\n",
    "noise_metric_relevant = NoiseSensitivity(llm=ragas_llm, mode=\"relevant\")\n",
    "\n",
    "ragas_noise = run_async(noise_metric_relevant.single_turn_ascore(noise_sample))\n",
    "\n",
    "print(\"ğŸ”¬ RAGAS Noise Sensitivity Result\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Mode: relevant\")\n",
    "print(f\"   Score: {ragas_noise:.2f}\")\n",
    "print(f\"\\n   Remember: Lower is better for this metric!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Interpretation Guide\n",
    "\n",
    "| Score | Meaning | What it means for your RAG |\n",
    "|-------|---------|---------------------------|\n",
    "| **0.0 - 0.2** | Excellent | Model effectively ignores irrelevant information |\n",
    "| **0.2 - 0.4** | Good | Occasional confusion but mostly robust |\n",
    "| **0.4 - 0.6** | Concerning | Model frequently picks up noise |\n",
    "| **0.6 - 1.0** | Poor | Model is highly susceptible to distraction |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ Section 8: Putting It All Together\n",
    "\n",
    "## 8.1 Metrics Relationship Diagram\n",
    "\n",
    "Understanding which metrics evaluate which component of your RAG system:\n",
    "\n",
    "```\n",
    "                           USER QUESTION\n",
    "                                â”‚\n",
    "                                â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚      RETRIEVER        â”‚\n",
    "                    â”‚                       â”‚\n",
    "                    â”‚  Metrics:             â”‚\n",
    "                    â”‚  â€¢ Context Precision  â”‚â—„â”€â”€ Is ranking good?\n",
    "                    â”‚  â€¢ Context Recall     â”‚â—„â”€â”€ Is coverage complete?\n",
    "                    â”‚  â€¢ Entity Recall      â”‚â—„â”€â”€ Are entities captured?\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                â”‚\n",
    "                                â–¼\n",
    "                       Retrieved Chunks\n",
    "                                â”‚\n",
    "                                â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚      GENERATOR        â”‚\n",
    "                    â”‚        (LLM)          â”‚\n",
    "                    â”‚                       â”‚\n",
    "                    â”‚  Metrics:             â”‚\n",
    "                    â”‚  â€¢ Faithfulness       â”‚â—„â”€â”€ No hallucinations?\n",
    "                    â”‚  â€¢ Noise Sensitivity  â”‚â—„â”€â”€ Ignores irrelevant?\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                â”‚\n",
    "                                â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚       ANSWER          â”‚\n",
    "                    â”‚                       â”‚\n",
    "                    â”‚  Metric:              â”‚\n",
    "                    â”‚  â€¢ Answer Relevancy   â”‚â—„â”€â”€ Addresses question?\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Complete Sample for Evaluation\n",
      "============================================================\n",
      "\n",
      "Question: What is the Eiffel Tower and where is it located?\n",
      "\n",
      "Response: The Eiffel Tower is a famous iron lattice tower located in Paris, France. It was built in 1889.\n",
      "\n",
      "Reference: The Eiffel Tower is a wrought-iron lattice tower in Paris, France. It was constructed from 1887 to 1889.\n",
      "\n",
      "Contexts: 4 chunks\n"
     ]
    }
   ],
   "source": [
    "# Complete evaluation with all 6 metrics\n",
    "\n",
    "# Create a comprehensive sample\n",
    "complete_sample = SingleTurnSample(\n",
    "    user_input=\"What is the Eiffel Tower and where is it located?\",\n",
    "    response=\"The Eiffel Tower is a famous iron lattice tower located in Paris, France. It was built in 1889.\",\n",
    "    reference=\"The Eiffel Tower is a wrought-iron lattice tower in Paris, France. It was constructed from 1887 to 1889.\",\n",
    "    retrieved_contexts=[\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
    "        \"The tower was constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair.\",\n",
    "        \"The Eiffel Tower is named after Gustave Eiffel, whose company designed and built the tower.\",\n",
    "        \"Paris is known for its cafe culture and fashion industry.\"  # Some noise\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š Complete Sample for Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nQuestion: {complete_sample.user_input}\")\n",
    "print(f\"\\nResponse: {complete_sample.response}\")\n",
    "print(f\"\\nReference: {complete_sample.reference}\")\n",
    "print(f\"\\nContexts: {len(complete_sample.retrieved_contexts)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Running All 6 RAGAS Metrics\n",
      "============================================================\n",
      "\n",
      "âœ… Faithfulness: 1.000 (higher is better)\n",
      "   Assessment: Good\n",
      "\n",
      "âœ… Answer Relevancy: 0.892 (higher is better)\n",
      "   Assessment: Good\n",
      "\n",
      "âœ… Context Precision: 1.000 (higher is better)\n",
      "   Assessment: Good\n",
      "\n",
      "âœ… Context Recall: 1.000 (higher is better)\n",
      "   Assessment: Good\n",
      "\n",
      "âœ… Context Entity Recall: 1.000 (higher is better)\n",
      "   Assessment: Good\n",
      "\n",
      "âœ… Noise Sensitivity: 0.000 (lower is better)\n",
      "   Assessment: Good\n"
     ]
    }
   ],
   "source": [
    "# Run all 6 metrics\n",
    "\n",
    "print(\"ğŸ”¬ Running All 6 RAGAS Metrics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize all metrics\n",
    "all_metrics = {\n",
    "    \"Faithfulness\": Faithfulness(llm=ragas_llm),\n",
    "    \"Answer Relevancy\": ResponseRelevancy(llm=ragas_llm, embeddings=ragas_embeddings),\n",
    "    \"Context Precision\": LLMContextPrecisionWithReference(llm=ragas_llm),\n",
    "    \"Context Recall\": LLMContextRecall(llm=ragas_llm),\n",
    "    \"Context Entity Recall\": ContextEntityRecall(llm=ragas_llm),\n",
    "    \"Noise Sensitivity\": NoiseSensitivity(llm=ragas_llm)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, metric in all_metrics.items():\n",
    "    try:\n",
    "        score = run_async(metric.single_turn_ascore(complete_sample))\n",
    "        results[name] = score\n",
    "        \n",
    "        # Interpret the score\n",
    "        if name == \"Noise Sensitivity\":\n",
    "            quality = \"Good\" if score < 0.3 else \"Concerning\" if score < 0.6 else \"Poor\"\n",
    "            direction = \"(lower is better)\"\n",
    "        else:\n",
    "            quality = \"Good\" if score > 0.7 else \"Concerning\" if score > 0.5 else \"Poor\"\n",
    "            direction = \"(higher is better)\"\n",
    "        \n",
    "        print(f\"\\nâœ… {name}: {score:.3f} {direction}\")\n",
    "        print(f\"   Assessment: {quality}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ {name}: Error - {str(e)[:50]}\")\n",
    "        results[name] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“Š EVALUATION SUMMARY\n",
      "======================================================================\n",
      "           Metric Score Ideal Status\n",
      "     Faithfulness 1.000   1.0      âœ…\n",
      " Answer Relevancy 0.838   1.0      âœ…\n",
      "Context Precision 1.000   1.0      âœ…\n",
      "   Context Recall 1.000   1.0      âœ…\n",
      "Noise Sensitivity 0.000   0.0      âœ…\n"
     ]
    }
   ],
   "source": [
    "# Summary table\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“Š EVALUATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_data = []\n",
    "for name, score in results.items():\n",
    "    if score is not None:\n",
    "        if name == \"Noise Sensitivity\":\n",
    "            ideal = \"0.0\"\n",
    "            status = \"âœ…\" if score < 0.3 else \"âš ï¸\" if score < 0.6 else \"âŒ\"\n",
    "        else:\n",
    "            ideal = \"1.0\"\n",
    "            status = \"âœ…\" if score > 0.7 else \"âš ï¸\" if score > 0.5 else \"âŒ\"\n",
    "        summary_data.append({\n",
    "            \"Metric\": name,\n",
    "            \"Score\": f\"{score:.3f}\",\n",
    "            \"Ideal\": ideal,\n",
    "            \"Status\": status\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(df_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Debugging Guide: What to Do When Scores Are Low\n",
    "\n",
    "| Metric | If Score is Low | Action |\n",
    "|--------|-----------------|--------|\n",
    "| **Faithfulness** | LLM is hallucinating | Improve prompt to emphasize context adherence, reduce temperature, use stronger LLM |\n",
    "| **Answer Relevancy** | Answer doesn't address the question | Review prompt template, ensure question type (when/who/what) is addressed |\n",
    "| **Context Precision** | Irrelevant chunks ranked high | Improve embedding model, add re-ranking, tune retriever similarity threshold |\n",
    "| **Context Recall** | Missing important information | Increase k (number of chunks), improve chunking strategy, enhance embedding quality |\n",
    "| **Entity Recall** | Key entities not retrieved | Use entity-aware chunking, keyword search hybrid, increase retrieval scope |\n",
    "| **Noise Sensitivity** | Model confused by noise (high score) | Filter irrelevant chunks, use re-ranker, improve prompt robustness |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸš€ Section 9: Production Patterns\n",
    "\n",
    "Now let's see how to use these metrics with a real RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Created 3 test samples for batch evaluation\n"
     ]
    }
   ],
   "source": [
    "# Create a batch of test samples\n",
    "\n",
    "test_samples = [\n",
    "    SingleTurnSample(\n",
    "        user_input=\"What is RAG?\",\n",
    "        response=\"RAG stands for Retrieval Augmented Generation. It combines retrieval systems with LLMs to provide accurate, grounded responses.\",\n",
    "        reference=\"RAG (Retrieval Augmented Generation) is a technique that enhances LLM responses by retrieving relevant documents and using them as context.\",\n",
    "        retrieved_contexts=[\n",
    "            \"RAG combines retrieval with generation for accurate responses.\",\n",
    "            \"Retrieval Augmented Generation uses external knowledge bases.\"\n",
    "        ]\n",
    "    ),\n",
    "    SingleTurnSample(\n",
    "        user_input=\"What are embeddings?\",\n",
    "        response=\"Embeddings are numerical vector representations of text that capture semantic meaning.\",\n",
    "        reference=\"Embeddings are dense vector representations that encode semantic information about text into numerical format.\",\n",
    "        retrieved_contexts=[\n",
    "            \"Embeddings convert text to dense vectors.\",\n",
    "            \"Vector representations capture semantic similarity.\"\n",
    "        ]\n",
    "    ),\n",
    "    SingleTurnSample(\n",
    "        user_input=\"What is chunking?\",\n",
    "        response=\"Chunking is the process of breaking documents into smaller pieces for processing.\",\n",
    "        reference=\"Chunking divides large documents into smaller segments that can be individually embedded and retrieved.\",\n",
    "        retrieved_contexts=[\n",
    "            \"Document chunking breaks text into manageable pieces.\",\n",
    "            \"Chunk size affects retrieval quality.\"\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“Š Created {len(test_samples)} test samples for batch evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Running Batch Evaluation...\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bae8cbc8c54d7dbbf7de80faf7a6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Batch evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Batch evaluation using EvaluationDataset\n",
    "\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "# Create evaluation dataset\n",
    "eval_dataset = EvaluationDataset(samples=test_samples)\n",
    "\n",
    "# Select metrics for batch evaluation\n",
    "batch_metrics = [\n",
    "    Faithfulness(llm=ragas_llm),\n",
    "    ResponseRelevancy(llm=ragas_llm, embeddings=ragas_embeddings),\n",
    "    LLMContextRecall(llm=ragas_llm)\n",
    "]\n",
    "\n",
    "print(\"ğŸ”¬ Running Batch Evaluation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run evaluation\n",
    "batch_results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=batch_metrics\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Batch evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Batch Evaluation Results\n",
      "============================================================\n",
      "             user_input                                                                                                               retrieved_contexts                                                                                                                         response                                                                                                                                    reference  faithfulness  answer_relevancy  context_recall\n",
      "0          What is RAG?  [RAG combines retrieval with generation for accurate responses., Retrieval Augmented Generation uses external knowledge bases.]  RAG stands for Retrieval Augmented Generation. It combines retrieval systems with LLMs to provide accurate, grounded responses.  RAG (Retrieval Augmented Generation) is a technique that enhances LLM responses by retrieving relevant documents and using them as context.      0.666667          0.915684             1.0\n",
      "1  What are embeddings?                                 [Embeddings convert text to dense vectors., Vector representations capture semantic similarity.]                                           Embeddings are numerical vector representations of text that capture semantic meaning.                               Embeddings are dense vector representations that encode semantic information about text into numerical format.      1.000000          0.814045             1.0\n",
      "2     What is chunking?                                   [Document chunking breaks text into manageable pieces., Chunk size affects retrieval quality.]                                                Chunking is the process of breaking documents into smaller pieces for processing.                                      Chunking divides large documents into smaller segments that can be individually embedded and retrieved.      0.500000          0.841499             0.0\n",
      "\n",
      "ğŸ“ˆ Average Scores:\n",
      "   faithfulness: 0.722\n",
      "   answer_relevancy: 0.857\n",
      "   context_recall: 0.667\n"
     ]
    }
   ],
   "source": [
    "# Display batch results\n",
    "\n",
    "print(\"ğŸ“Š Batch Evaluation Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = batch_results.to_pandas()\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Calculate averages\n",
    "print(\"\\nğŸ“ˆ Average Scores:\")\n",
    "for col in results_df.columns:\n",
    "    if col not in ['user_input', 'response', 'reference', 'retrieved_contexts']:\n",
    "        avg = results_df[col].mean()\n",
    "        print(f\"   {col}: {avg:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ Section 10: Practice Exercises\n",
    "\n",
    "Test your understanding with these hands-on exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Faithfulness Analysis\n",
    "\n",
    "Given this response and context, manually identify claims and predict the Faithfulness score:\n",
    "\n",
    "**Response:** \"Python was created by Guido van Rossum in 1991. It is known for its elegant syntax and is the most popular programming language in 2024.\"\n",
    "\n",
    "**Context:** \"Python is a high-level programming language created by Guido van Rossum. It was first released in 1991.\"\n",
    "\n",
    "**Your task:**\n",
    "1. Extract all claims from the response\n",
    "2. Verify each claim against the context\n",
    "3. Calculate the expected Faithfulness score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - Your solution here\n",
    "\n",
    "# TODO: Extract claims and calculate faithfulness\n",
    "exercise1_claims = [\n",
    "    # Add your claims here\n",
    "    # (\"claim text\", True/False for supported)\n",
    "]\n",
    "\n",
    "# Verify your answer\n",
    "exercise1_sample = SingleTurnSample(\n",
    "    user_input=\"Tell me about Python\",\n",
    "    response=\"Python was created by Guido van Rossum in 1991. It is known for its elegant syntax and is the most popular programming language in 2024.\",\n",
    "    retrieved_contexts=[\"Python is a high-level programming language created by Guido van Rossum. It was first released in 1991.\"]\n",
    ")\n",
    "\n",
    "# Uncomment to check your answer:\n",
    "# score = run_async(faithfulness_metric.single_turn_ascore(exercise1_sample))\n",
    "# print(f\"Actual Faithfulness score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Answer Relevancy Prediction\n",
    "\n",
    "For each question-answer pair, predict whether the relevancy will be HIGH (>0.8) or LOW (<0.5):\n",
    "\n",
    "1. Q: \"Who invented the telephone?\" A: \"Alexander Graham Bell invented the telephone in 1876.\"\n",
    "2. Q: \"Who invented the telephone?\" A: \"The telephone revolutionized communication worldwide.\"\n",
    "3. Q: \"When was the moon landing?\" A: \"Neil Armstrong was the first person to walk on the moon.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Test your predictions\n",
    "\n",
    "exercise2_pairs = [\n",
    "    (\"Who invented the telephone?\", \"Alexander Graham Bell invented the telephone in 1876.\"),\n",
    "    (\"Who invented the telephone?\", \"The telephone revolutionized communication worldwide.\"),\n",
    "    (\"When was the moon landing?\", \"Neil Armstrong was the first person to walk on the moon.\")\n",
    "]\n",
    "\n",
    "# Your predictions: \n",
    "# 1. HIGH / LOW?\n",
    "# 2. HIGH / LOW?\n",
    "# 3. HIGH / LOW?\n",
    "\n",
    "# Uncomment to verify:\n",
    "# for q, a in exercise2_pairs:\n",
    "#     sample = SingleTurnSample(user_input=q, response=a, retrieved_contexts=[\"context\"])\n",
    "#     score = run_async(relevancy_metric.single_turn_ascore(sample))\n",
    "#     print(f\"Q: {q[:40]}... Score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Context Precision - Ranking Impact\n",
    "\n",
    "Given these 4 chunks for the question \"What is the capital of Japan?\", arrange them to get:\n",
    "\n",
    "**Chunks:**\n",
    "- A: \"Tokyo is the capital of Japan.\"\n",
    "- B: \"Japan has a population of 125 million.\"\n",
    "- C: \"Japan is an island nation in East Asia.\"\n",
    "- D: \"Sushi is a popular Japanese dish.\"\n",
    "\n",
    "1. **Maximize** Context Precision (best possible score)\n",
    "2. **Minimize** Context Precision (worst possible score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 - Test your rankings\n",
    "\n",
    "chunks = {\n",
    "    \"A\": \"Tokyo is the capital of Japan.\",\n",
    "    \"B\": \"Japan has a population of 125 million.\",\n",
    "    \"C\": \"Japan is an island nation in East Asia.\",\n",
    "    \"D\": \"Sushi is a popular Japanese dish.\"\n",
    "}\n",
    "\n",
    "# Your best ranking (e.g., [\"A\", \"C\", \"B\", \"D\"]):\n",
    "best_ranking = []  # Fill in your answer\n",
    "\n",
    "# Your worst ranking:\n",
    "worst_ranking = []  # Fill in your answer\n",
    "\n",
    "# Uncomment to test:\n",
    "# def test_ranking(ranking):\n",
    "#     sample = SingleTurnSample(\n",
    "#         user_input=\"What is the capital of Japan?\",\n",
    "#         reference=\"Tokyo is the capital of Japan.\",\n",
    "#         retrieved_contexts=[chunks[c] for c in ranking]\n",
    "#     )\n",
    "#     return run_async(precision_metric.single_turn_ascore(sample))\n",
    "# \n",
    "# print(f\"Best ranking score: {test_ranking(best_ranking):.2f}\")\n",
    "# print(f\"Worst ranking score: {test_ranking(worst_ranking):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“š Summary & Key Takeaways\n",
    "\n",
    "## Quick Reference Table\n",
    "\n",
    "| Metric | What It Measures | Calculation | Ideal |\n",
    "|--------|-----------------|-------------|-------|\n",
    "| **Faithfulness** | Hallucination detection | Supported claims / Total claims | 1.0 |\n",
    "| **Answer Relevancy** | Answer addresses question | Avg cosine similarity of generated Qs | 1.0 |\n",
    "| **Context Precision** | Ranking quality | Position-weighted precision | 1.0 |\n",
    "| **Context Recall** | Retrieval completeness | Reference claims found / Total | 1.0 |\n",
    "| **Entity Recall** | Entity coverage | Common entities / Reference entities | 1.0 |\n",
    "| **Noise Sensitivity** | Robustness to noise | Incorrect claims / Total claims | 0.0 |\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "1. **Faithfulness** and **Answer Relevancy** evaluate the **Generator (LLM)**\n",
    "2. **Context Precision**, **Context Recall**, and **Entity Recall** evaluate the **Retriever**\n",
    "3. **Noise Sensitivity** evaluates the **overall system's robustness**\n",
    "4. Understanding the **intermediate steps** helps debug evaluation issues\n",
    "5. Use these metrics **together** for comprehensive RAG evaluation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- ğŸ“– Review Notebook 10 for practical RAGAS workflows\n",
    "- ğŸ”§ Apply these metrics to your own RAG pipeline\n",
    "- ğŸ“Š Set up automated evaluation with thresholds\n",
    "- ğŸ”¬ Experiment with different LLMs and compare scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Additional Resources\n",
    "\n",
    "- [RAGAS Official Documentation](https://docs.ragas.io/)\n",
    "- [RAGAS GitHub Repository](https://github.com/explodinggradients/ragas)\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook created for the  RAGAS Metrics Deep Dive**\n",
    "\n",
    "*Last updated: Nov 2025*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
